{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "51zlDU-VPXhr"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries and packages\n",
        "import json \n",
        "import numpy as np \n",
        "import random\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "kjI0qk25RY_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "with open('intents.json') as file:\n",
        "  data = json.load(file)\n",
        "\n",
        "# Hold all the training data    \n",
        "training_sentences = []\n",
        "#Hold all target labels that cprrespond to each training data\n",
        "training_labels = []\n",
        "labels = []\n",
        "responses = []\n",
        "\n",
        "for intent in data['intents']:\n",
        "    for question in intent['questions']:\n",
        "        training_sentences.append(question)\n",
        "        training_labels.append(intent['tag'])\n",
        "    responses.append(intent['responses'])\n",
        "    \n",
        "    if intent['tag'] not in labels:\n",
        "        labels.append(intent['tag'])\n",
        "        \n",
        "num_classes = len(labels)"
      ],
      "metadata": {
        "id": "36CXOtgTzkK6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to get the best match for a given question\n",
        "def get_best_match(question):\n",
        "    # Set a threshold for the minimum score\n",
        "    threshold = 70\n",
        "    \n",
        "    # Initialize variables to store the best match and its score\n",
        "    best_match = None\n",
        "    best_score = 0\n",
        "    \n",
        "    # Loop through all the questions in the data\n",
        "    for item in data:\n",
        "        # Compute the score for the current question\n",
        "        score = fuzz.token_set_ratio(question, item['question'])\n",
        "        \n",
        "        # If the score is above the threshold and better than the previous best score, update the best match\n",
        "        if score > threshold and score > best_score:\n",
        "            best_match = item['responses']\n",
        "            best_score = score\n",
        "    \n",
        "    # If there is a best match, return it. Otherwise, return a default message.\n",
        "    if best_match is not None:\n",
        "        return best_match\n",
        "    else:\n",
        "        return \"I'm sorry, I don't understand your question.\""
      ],
      "metadata": {
        "id": "npgyMHEvvADE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To convert target label into a model understandable form\n",
        "lbl_encoder = LabelEncoder()\n",
        "lbl_encoder.fit(training_labels)\n",
        "training_labels = lbl_encoder.transform(training_labels)"
      ],
      "metadata": {
        "id": "0I1g-tiQzvim"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_len = 20\n",
        "oov_token = \"<OOV>\"\n",
        "# To vectorize the whole text data \n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "# to make all the training text sequence into the same size\n",
        "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)"
      ],
      "metadata": {
        "id": "zZ7BqoTK07c3"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Design and Training"
      ],
      "metadata": {
        "id": "fH8EqozISLHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "\n",
        "model.add(SimpleRNN(16))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', \n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Applying earlystopping to avoid model overfitting\n",
        "earlystop_callback = EarlyStopping(monitor='loss', patience=5)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWKGX8NK1CZx",
        "outputId": "73158b05-17ed-43f7-b595-56089635c9bc"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 20, 16)            16000     \n",
            "                                                                 \n",
            " simple_rnn_4 (SimpleRNN)    (None, 16)                528       \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 72)                1224      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,296\n",
            "Trainable params: 18,296\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 150\n",
        "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs, callbacks=[earlystop_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntzj2Sie1Sw-",
        "outputId": "8c25bcde-e262-4266-9e1f-a1b129fac5d5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "8/8 [==============================] - 3s 145ms/step - loss: 4.2802 - accuracy: 0.0000e+00\n",
            "Epoch 2/150\n",
            "8/8 [==============================] - 1s 170ms/step - loss: 4.2721 - accuracy: 0.0088\n",
            "Epoch 3/150\n",
            "8/8 [==============================] - 0s 66ms/step - loss: 4.2628 - accuracy: 0.0354\n",
            "Epoch 4/150\n",
            "8/8 [==============================] - 1s 86ms/step - loss: 4.2540 - accuracy: 0.0398\n",
            "Epoch 5/150\n",
            "8/8 [==============================] - 0s 65ms/step - loss: 4.2407 - accuracy: 0.0708\n",
            "Epoch 6/150\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 4.2248 - accuracy: 0.0841\n",
            "Epoch 7/150\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 4.2041 - accuracy: 0.0885\n",
            "Epoch 8/150\n",
            "8/8 [==============================] - 0s 42ms/step - loss: 4.1767 - accuracy: 0.0841\n",
            "Epoch 9/150\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 4.1465 - accuracy: 0.1062\n",
            "Epoch 10/150\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 4.1080 - accuracy: 0.0664\n",
            "Epoch 11/150\n",
            "8/8 [==============================] - 0s 62ms/step - loss: 4.0567 - accuracy: 0.0664\n",
            "Epoch 12/150\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 4.0002 - accuracy: 0.0708\n",
            "Epoch 13/150\n",
            "8/8 [==============================] - 1s 83ms/step - loss: 3.9358 - accuracy: 0.0575\n",
            "Epoch 14/150\n",
            "8/8 [==============================] - 1s 83ms/step - loss: 3.8611 - accuracy: 0.0664\n",
            "Epoch 15/150\n",
            "8/8 [==============================] - 0s 51ms/step - loss: 3.7934 - accuracy: 0.0531\n",
            "Epoch 16/150\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 3.7149 - accuracy: 0.0885\n",
            "Epoch 17/150\n",
            "8/8 [==============================] - 0s 41ms/step - loss: 3.6311 - accuracy: 0.0752\n",
            "Epoch 18/150\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 3.5471 - accuracy: 0.0973\n",
            "Epoch 19/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 3.4661 - accuracy: 0.1150\n",
            "Epoch 20/150\n",
            "8/8 [==============================] - 0s 63ms/step - loss: 3.3827 - accuracy: 0.1239\n",
            "Epoch 21/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 3.3013 - accuracy: 0.1504\n",
            "Epoch 22/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 3.2186 - accuracy: 0.1903\n",
            "Epoch 23/150\n",
            "8/8 [==============================] - 0s 43ms/step - loss: 3.1313 - accuracy: 0.2212\n",
            "Epoch 24/150\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 3.0392 - accuracy: 0.2434\n",
            "Epoch 25/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 2.9537 - accuracy: 0.2566\n",
            "Epoch 26/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 2.8681 - accuracy: 0.2743\n",
            "Epoch 27/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 2.7858 - accuracy: 0.3186\n",
            "Epoch 28/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 2.7035 - accuracy: 0.3319\n",
            "Epoch 29/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 2.6133 - accuracy: 0.3230\n",
            "Epoch 30/150\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 2.5341 - accuracy: 0.3142\n",
            "Epoch 31/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 2.4507 - accuracy: 0.3407\n",
            "Epoch 32/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 2.3770 - accuracy: 0.3628\n",
            "Epoch 33/150\n",
            "8/8 [==============================] - 0s 43ms/step - loss: 2.2980 - accuracy: 0.4248\n",
            "Epoch 34/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 2.2132 - accuracy: 0.4735\n",
            "Epoch 35/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 2.1307 - accuracy: 0.5265\n",
            "Epoch 36/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 2.0467 - accuracy: 0.5531\n",
            "Epoch 37/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 1.9648 - accuracy: 0.5929\n",
            "Epoch 38/150\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 1.8852 - accuracy: 0.6637\n",
            "Epoch 39/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 1.8153 - accuracy: 0.6947\n",
            "Epoch 40/150\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 1.7476 - accuracy: 0.6858\n",
            "Epoch 41/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 1.6738 - accuracy: 0.7212\n",
            "Epoch 42/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 1.6169 - accuracy: 0.7212\n",
            "Epoch 43/150\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 1.5510 - accuracy: 0.7522\n",
            "Epoch 44/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 1.5011 - accuracy: 0.7566\n",
            "Epoch 45/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 1.4359 - accuracy: 0.7876\n",
            "Epoch 46/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 1.3827 - accuracy: 0.8009\n",
            "Epoch 47/150\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 1.3145 - accuracy: 0.8053\n",
            "Epoch 48/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 1.2562 - accuracy: 0.8540\n",
            "Epoch 49/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 1.2108 - accuracy: 0.8761\n",
            "Epoch 50/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 1.1574 - accuracy: 0.9159\n",
            "Epoch 51/150\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 1.1043 - accuracy: 0.9425\n",
            "Epoch 52/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 1.0867 - accuracy: 0.9336\n",
            "Epoch 53/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 1.0260 - accuracy: 0.9558\n",
            "Epoch 54/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.9762 - accuracy: 0.9513\n",
            "Epoch 55/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.9326 - accuracy: 0.9602\n",
            "Epoch 56/150\n",
            "8/8 [==============================] - 0s 43ms/step - loss: 0.8889 - accuracy: 0.9646\n",
            "Epoch 57/150\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.8528 - accuracy: 0.9602\n",
            "Epoch 58/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.8594 - accuracy: 0.9646\n",
            "Epoch 59/150\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 0.7976 - accuracy: 0.9646\n",
            "Epoch 60/150\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 0.7619 - accuracy: 0.9558\n",
            "Epoch 61/150\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 0.7185 - accuracy: 0.9558\n",
            "Epoch 62/150\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 0.6868 - accuracy: 0.9558\n",
            "Epoch 63/150\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.6777 - accuracy: 0.9602\n",
            "Epoch 64/150\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 0.6483 - accuracy: 0.9602\n",
            "Epoch 65/150\n",
            "8/8 [==============================] - 0s 53ms/step - loss: 0.6094 - accuracy: 0.9602\n",
            "Epoch 66/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.5773 - accuracy: 0.9823\n",
            "Epoch 67/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.5518 - accuracy: 0.9779\n",
            "Epoch 68/150\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.5231 - accuracy: 0.9867\n",
            "Epoch 69/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.5024 - accuracy: 0.9735\n",
            "Epoch 70/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4783 - accuracy: 0.9779\n",
            "Epoch 71/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.4556 - accuracy: 0.9779\n",
            "Epoch 72/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.4355 - accuracy: 0.9779\n",
            "Epoch 73/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.4217 - accuracy: 0.9823\n",
            "Epoch 74/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4025 - accuracy: 0.9735\n",
            "Epoch 75/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.4232 - accuracy: 0.9690\n",
            "Epoch 76/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3896 - accuracy: 0.9735\n",
            "Epoch 77/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3639 - accuracy: 0.9823\n",
            "Epoch 78/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.3451 - accuracy: 0.9867\n",
            "Epoch 79/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3242 - accuracy: 0.9867\n",
            "Epoch 80/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.3085 - accuracy: 0.9867\n",
            "Epoch 81/150\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 0.2934 - accuracy: 0.9867\n",
            "Epoch 82/150\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.2821 - accuracy: 0.9867\n",
            "Epoch 83/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2778 - accuracy: 0.9823\n",
            "Epoch 84/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2665 - accuracy: 0.9823\n",
            "Epoch 85/150\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2564 - accuracy: 0.9867\n",
            "Epoch 86/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2481 - accuracy: 0.9912\n",
            "Epoch 87/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2361 - accuracy: 0.9912\n",
            "Epoch 88/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2268 - accuracy: 0.9956\n",
            "Epoch 89/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2139 - accuracy: 0.9956\n",
            "Epoch 90/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2058 - accuracy: 0.9956\n",
            "Epoch 91/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.1960 - accuracy: 1.0000\n",
            "Epoch 92/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.1883 - accuracy: 0.9956\n",
            "Epoch 93/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.1815 - accuracy: 1.0000\n",
            "Epoch 94/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.1749 - accuracy: 1.0000\n",
            "Epoch 95/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.1674 - accuracy: 0.9956\n",
            "Epoch 96/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.1612 - accuracy: 0.9956\n",
            "Epoch 97/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.1553 - accuracy: 1.0000\n",
            "Epoch 98/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.1489 - accuracy: 1.0000\n",
            "Epoch 99/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.1437 - accuracy: 1.0000\n",
            "Epoch 100/150\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.1386 - accuracy: 1.0000\n",
            "Epoch 101/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.1329 - accuracy: 1.0000\n",
            "Epoch 102/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1296 - accuracy: 1.0000\n",
            "Epoch 103/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.1267 - accuracy: 1.0000\n",
            "Epoch 104/150\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.1205 - accuracy: 1.0000\n",
            "Epoch 105/150\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.1166 - accuracy: 1.0000\n",
            "Epoch 106/150\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.1122 - accuracy: 1.0000\n",
            "Epoch 107/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1083 - accuracy: 1.0000\n",
            "Epoch 108/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1047 - accuracy: 1.0000\n",
            "Epoch 109/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1023 - accuracy: 1.0000\n",
            "Epoch 110/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.1003 - accuracy: 1.0000\n",
            "Epoch 111/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0963 - accuracy: 1.0000\n",
            "Epoch 112/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0925 - accuracy: 1.0000\n",
            "Epoch 113/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0896 - accuracy: 1.0000\n",
            "Epoch 114/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0866 - accuracy: 1.0000\n",
            "Epoch 115/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0843 - accuracy: 1.0000\n",
            "Epoch 116/150\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 0.0822 - accuracy: 1.0000\n",
            "Epoch 117/150\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 0.0827 - accuracy: 0.9956\n",
            "Epoch 118/150\n",
            "8/8 [==============================] - 0s 55ms/step - loss: 0.0800 - accuracy: 0.9956\n",
            "Epoch 119/150\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 0.0771 - accuracy: 1.0000\n",
            "Epoch 120/150\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 0.0733 - accuracy: 1.0000\n",
            "Epoch 121/150\n",
            "8/8 [==============================] - 0s 42ms/step - loss: 0.0710 - accuracy: 1.0000\n",
            "Epoch 122/150\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.0687 - accuracy: 1.0000\n",
            "Epoch 123/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0668 - accuracy: 1.0000\n",
            "Epoch 124/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0648 - accuracy: 1.0000\n",
            "Epoch 125/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0631 - accuracy: 1.0000\n",
            "Epoch 126/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0616 - accuracy: 1.0000\n",
            "Epoch 127/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0616 - accuracy: 1.0000\n",
            "Epoch 128/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0593 - accuracy: 1.0000\n",
            "Epoch 129/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0585 - accuracy: 1.0000\n",
            "Epoch 130/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0561 - accuracy: 1.0000\n",
            "Epoch 131/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0545 - accuracy: 1.0000\n",
            "Epoch 132/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0530 - accuracy: 1.0000\n",
            "Epoch 133/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0515 - accuracy: 1.0000\n",
            "Epoch 134/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0502 - accuracy: 1.0000\n",
            "Epoch 135/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0495 - accuracy: 1.0000\n",
            "Epoch 136/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0483 - accuracy: 1.0000\n",
            "Epoch 137/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0473 - accuracy: 1.0000\n",
            "Epoch 138/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0469 - accuracy: 1.0000\n",
            "Epoch 139/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0448 - accuracy: 1.0000\n",
            "Epoch 140/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0437 - accuracy: 1.0000\n",
            "Epoch 141/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0427 - accuracy: 1.0000\n",
            "Epoch 142/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0417 - accuracy: 1.0000\n",
            "Epoch 143/150\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0408 - accuracy: 1.0000\n",
            "Epoch 144/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0397 - accuracy: 1.0000\n",
            "Epoch 145/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0396 - accuracy: 1.0000\n",
            "Epoch 146/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0385 - accuracy: 1.0000\n",
            "Epoch 147/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0376 - accuracy: 1.0000\n",
            "Epoch 148/150\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0363 - accuracy: 1.0000\n",
            "Epoch 149/150\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0355 - accuracy: 1.0000\n",
            "Epoch 150/150\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0348 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to save the trained model\n",
        "model.save(\"chat_model\")\n",
        "\n",
        "# to save the fitted tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "# to save the fitted label encoder\n",
        "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
        "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "JdORiPQS1eYa"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Testing and Evaluation"
      ],
      "metadata": {
        "id": "u_3W9psySSjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    # load trained model\n",
        "    model = keras.models.load_model('chat_model')\n",
        "\n",
        "    # load tokenizer object\n",
        "    with open('tokenizer.pickle', 'rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "\n",
        "    # load label encoder object\n",
        "    with open('label_encoder.pickle', 'rb') as enc:\n",
        "        lbl_encoder = pickle.load(enc)\n",
        "\n",
        "    # parameters\n",
        "    max_len = 20\n",
        "    \n",
        "    while True:\n",
        "        print(\"User: \", end=\"\")\n",
        "        inp = input()\n",
        "        if inp.lower() == \"quit\":\n",
        "          print(\"AruBot: Goodbye!\")\n",
        "          break\n",
        "        \n",
        "        result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]),\n",
        "                                             truncating='post', maxlen=max_len))\n",
        "        tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
        "        for i in data['intents']:\n",
        "          if i['tag'] == tag:\n",
        "            print(\"AruBot:\", np.random.choice(i['responses']))\n",
        "\n",
        "print(\"Start messaging with the bot (type quit to stop)!\")\n",
        "chat()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkYbwLOX2Gtc",
        "outputId": "98d39442-2ee6-4940-d549-38d36da8ab25"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start messaging with the bot (type quit to stop)!\n",
            "User: hello\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "AruBot: Hi\n",
            "User: quit\n",
            "AruBot: Goodbye!\n"
          ]
        }
      ]
    }
  ]
}